{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a745f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e34a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed9816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Environment\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "class Environment():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def FrozenLakeNoSlippery(self):\n",
    "        register(\n",
    "                 id= 'FrozenLakeNoSlippery-v0',\n",
    "                 entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "                 kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "                 max_episode_steps=100,\n",
    "                 reward_threshold=0.82\n",
    "                 )\n",
    "        env = gym.make('FrozenLakeNoSlippery-v0')\n",
    "        return env\n",
    "\n",
    "\n",
    "class DeepQAgent():\n",
    "    \n",
    "    def __init__(self, args, env):\n",
    "        \n",
    "        # set hyperparameters\n",
    "        self.max_episodes = 20000\n",
    "        self.max_actions = 99\n",
    "        self.discount = 0.93\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_decay = 1.0/20000\n",
    "        # get envirionment\n",
    "        self.env = env\n",
    "    \n",
    "        # nn_model parameters\n",
    "        self.in_units = env.observation_space.n\n",
    "        self.out_units = env.action_space.n\n",
    "        self.hidden_units = 10 #int(args.hidden_units)\n",
    "        \n",
    "        # construct nn model\n",
    "        self._nn_model()\n",
    "    \n",
    "        # save nn model\n",
    "        self.saver = tf.train.Saver()\n",
    " \n",
    "    def _nn_model(self):\n",
    "        self.a0 = tf.placeholder(tf.float32, shape=[1, self.in_units]) # input layer\n",
    "        self.y = tf.placeholder(tf.float32, shape=[1, self.out_units]) # ouput layer\n",
    "        \n",
    "        # from input layer to hidden layer\n",
    "        w1 = tf.Variable(tf.zeros([self.in_units, self.hidden_units], dtype=tf.float32), name='w1') # weight\n",
    "        b1 = tf.Variable(tf.random_uniform([self.hidden_units], 0, 0.01, dtype=tf.float32), name='b1') # bias\n",
    "        a1 = tf.nn.relu(tf.matmul(self.a0, w1) + b1) # the ouput of hidden layer\n",
    "        \n",
    "        # from hidden layer to output layer\n",
    "        w2 = tf.Variable(tf.zeros([self.hidden_units, self.out_units], dtype=tf.float32), name='w2') # weight\n",
    "        b2 = tf.Variable(tf.random_uniform([self.out_units], 0, 0.01, dtype=tf.float32), name='b2') # bias\n",
    "        \n",
    "        # Q-value and Action\n",
    "        self.a2 = tf.matmul(a1, w2) + b2 # the predicted_y (Q-value) of four actions\n",
    "        self.action = tf.argmax(self.a2, 1) # the agent would take the action which has maximum Q-value\n",
    "        \n",
    "        # loss function\n",
    "        loss = tf.reduce_sum(tf.square(self.a2-self.y))\n",
    "        \n",
    "        # upate model, minimizing loss function\n",
    "        self.update_model =  tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(loss)\n",
    "    \n",
    "    def train(self):\n",
    "        # hyper parameter\n",
    "        max_episodes = self.max_episodes\n",
    "        max_actions = self.max_actions\n",
    "        discount = self.discount\n",
    "        exploration_rate = self.exploration_rate\n",
    "        exploration_decay = self.exploration_decay\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(max_episodes):\n",
    "                state = env.reset()\n",
    "                for j in range(max_actions):\n",
    "                    action, pred_Q = sess.run([self.action, self.a2],feed_dict={self.a0:np.eye(self.in_units)[state:state+1]})\n",
    "                    \n",
    "                    if np.random.rand()<exploration_rate: # exploration\n",
    "                        action[0] = env.action_space.sample() # take a random action\n",
    "\n",
    "                    next_state, rewards, done, info = env.step(action[0])\n",
    "                    next_Q = sess.run(self.a2,feed_dict={self.a0:np.eye(self.in_units)[next_state:next_state+1]})\n",
    "\n",
    "                    update_Q = pred_Q\n",
    "                    update_Q [0,action[0]] = rewards + discount*np.max(next_Q)\n",
    "                    \n",
    "                    sess.run([self.update_model],\n",
    "                             feed_dict={self.a0:np.identity(16)[state:state+1],self.y:update_Q})\n",
    "                    state = next_state\n",
    "                    \n",
    "                    if done:\n",
    "                        if exploration_rate > 0.001:\n",
    "                            exploration_rate -= exploration_decay\n",
    "                        break\n",
    "            save_path = self.saver.save(sess, \"./nn_model.ckpt\")\n",
    "\n",
    "    def test(self):\n",
    "        max_actions = self.max_actions # hyper-parameter\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver=tf.train.import_meta_graph(\"./nn_model.ckpt.meta\") # restore model\n",
    "            saver.restore(sess, tf.train.latest_checkpoint('./'))# 載入參數\n",
    "            state = env.reset()\n",
    "            for j in range(max_actions):\n",
    "                env.render()\n",
    "                action, pred_Q = sess.run([self.action, self.a2],feed_dict={self.a0:np.eye(self.in_units)[state:state+1]})\n",
    "                next_state, rewards, done, info = env.step(action[0])\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    env.render()\n",
    "                    break\n",
    "\n",
    "def arg_parse():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--max_episodes\", help=\"max training episode\", default=20000)\n",
    "    parser.add_argument(\"--max_actions\", help=\"max actions per episode\", default=99)\n",
    "    parser.add_argument(\"--discount\", help=\"discount factpr for Q-learning\", default=0.95)\n",
    "    parser.add_argument(\"--exploration_rate\", help=\"exploration_rate\", default=1.0)\n",
    "    parser.add_argument(\"--hidden_units\", help=\"hidden units\", default=10)\n",
    "    return parser.parse_args()\n",
    "\n",
    "from easydict import EasyDict\n",
    "#args = EasyDict({\"--max_episodes\": 20000,\"--max_actions\": 99, \"--discount\":0.93, \"--exploration_rate\": 1.0,\n",
    "#                          \"--hidden_units\":10 })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = EasyDict({\"--max_episodes\": 20000,\"--max_actions\": 99, \"--discount\":0.93, \"--exploration_rate\": 1.0,\n",
    "                          \"--hidden_units\":10 })\n",
    "    env = Environment().FrozenLakeNoSlippery() # construct the environment\n",
    "    agent = DeepQAgent(args, env) # get agent\n",
    "    print(\"START TRAINING...\")\n",
    "    agent.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a93b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"\\n==============\\nTEST==============\\n\")\n",
    "    agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa709b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee45dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
