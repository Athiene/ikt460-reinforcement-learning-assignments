{
 "cells": [
  {
   "cell_type": "code",
   "id": "75abc13c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:05:30.294884Z",
     "start_time": "2025-02-19T12:05:30.291281Z"
    }
   },
   "source": [
    "# install the gym module that contains the NChain environment\n",
    "#!pip install gym==0.13.1"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d71d8314",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:05:34.879316Z",
     "start_time": "2025-02-19T12:05:32.370615Z"
    }
   },
   "source": [
    "# load the necessary python modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import random\n",
    "import gym\n",
    "import sys\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# ignore warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "# initialize the nchain environment\n",
    "env = gym.make('NChain-v0')\n",
    "# get 10 randomly sampled actions\n",
    "[env.action_space.sample() for _ in range(10)]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 0, 0, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a70b1b01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:05:38.624177Z",
     "start_time": "2025-02-19T12:05:38.619866Z"
    }
   },
   "source": [
    "# get 10 randomly sampled states\n",
    "[env.observation_space.sample() for _ in range(10)]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 0, 2, 2, 1, 0, 3, 4, 3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "a4171bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:05:40.913098Z",
     "start_time": "2025-02-19T12:05:40.905187Z"
    }
   },
   "source": [
    "class Qagent(object):\n",
    "    \"\"\"\n",
    "    Implementation of a Q-learning Algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        _action_size: int,\n",
    "        _state_size: int,\n",
    "        _learning_parameters: dict,\n",
    "        _exploration_parameters: dict,\n",
    "        _name: str = \"agent\",\n",
    "        _color: str = \"r\",\n",
    "    ) -> None:\n",
    "        \"\"\" initialize the q-learning agent\n",
    "\n",
    "        Args:\n",
    "            _action_size (int): number of actions the agent can take\n",
    "            _state_size (int): number of states the env has\n",
    "            _learning_parameters (dict): learning parameters of the agent\n",
    "            _exploration_parameters (dict): exploration parameters for the agent\n",
    "            _name (str, optional):  set the name of the Q-Agent. Defaults to \"agent\".\n",
    "            _color (str, optional): set the color of the agent for plotting. Defaults to \"r\".\n",
    "        \"\"\"\n",
    "        self.name = _name\n",
    "        self.color = _color\n",
    "\n",
    "        self.action_size = _action_size\n",
    "        self.state_size = _state_size\n",
    "        self.qtable = np.zeros((_state_size, _action_size))\n",
    "\n",
    "        self.learning_rate = _learning_parameters[\"learning_rate\"]\n",
    "        self.gamma = _learning_parameters[\"gamma\"]\n",
    "\n",
    "        self.epsilon = _exploration_parameters[\"epsilon\"]\n",
    "        self.max_epsilon = _exploration_parameters[\"max_epsilon\"]\n",
    "        self.min_epsilon = _exploration_parameters[\"min_epsilon\"]\n",
    "        self.decay_rate = _exploration_parameters[\"decay_rate\"]\n",
    "\n",
    "    def update_qtable(\n",
    "        self, state: int, new_state: int, action: int, reward: int, done: bool\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        update the q-table: Q(s,a) = Q(s,a) + lr  * [R(s,a) + gamma * max Q(s',a') - Q (s,a)]\n",
    "\n",
    "        Args:\n",
    "          state (int): current state of the environment\n",
    "          new_state (int): new state of the environment\n",
    "          action (int): current action taken by agent\n",
    "          reward (int): current reward received from env\n",
    "          done (boolean): variable indicating if env is done\n",
    "        \"\"\"\n",
    "        new_qvalue = (\n",
    "            reward\n",
    "            + self.gamma * np.max(self.qtable[new_state, :]) * (1 - done)\n",
    "            - self.qtable[state, action]\n",
    "        )\n",
    "        self.qtable[state, action] = (\n",
    "            self.qtable[state, action] + self.learning_rate * new_qvalue\n",
    "        )\n",
    "\n",
    "    def update_epsilon(self, episode: int) -> None:\n",
    "        \"\"\"\n",
    "        reduce epsilon, exponential decay\n",
    "\n",
    "        Args:\n",
    "          episode (int): number of episode\n",
    "        \"\"\"\n",
    "        self.epsilon = self.min_epsilon + (\n",
    "            self.max_epsilon - self.min_epsilon\n",
    "        ) * np.exp(-self.decay_rate * episode)\n",
    "\n",
    "    def get_action(self, state: int) -> int:\n",
    "        \"\"\"\n",
    "        select action e-greedy.exploration-exploitation trade-off:\n",
    "        - exploitation, max value for given state\n",
    "        - exploration, random choice\n",
    "\n",
    "        Args:\n",
    "          state (int): current state of the environment/agent\n",
    "\n",
    "        Returns:\n",
    "          action (int): action that the agent will take in the next step\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) >= self.epsilon:\n",
    "            action = np.argmax(self.qtable[state, :])\n",
    "        else:\n",
    "            action = np.random.choice(self.action_size)\n",
    "        return action\n",
    "\n",
    "    def __str__(self, tablefmt=\"fancy_grid\") -> str:\n",
    "        \"\"\"plot the q-table. Generate the table in fancy format.\n",
    "        \"\"\"\n",
    "        headers = [f\"Action {action}\" for action in range(self.action_size)]\n",
    "        show_index = [f\"State {state}\" for state in range(self.state_size)]\n",
    "        table = tabulate(self.qtable, headers=headers, showindex=show_index, tablefmt=\"fancy_grid\")\n",
    "        return f\"{self.name}\\n{table}\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "bc356dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:05:42.644567Z",
     "start_time": "2025-02-19T12:05:42.638544Z"
    }
   },
   "source": [
    "def learn_to_play(agent: Qagent, _max_game_steps: int = 10, _total_episodes: int = 1000) -> Qagent:\n",
    "    \"\"\"\n",
    "    implementation of the q-learning algorithm, here the q-table values are calculated\n",
    "\n",
    "    Args:\n",
    "      _max_game_steps (int): number of steps an agent can take, before the environment is reset\n",
    "      _total_episodes (int): total of training episodes (the number of trials an agent can do)\n",
    "      agent (Qagent):\n",
    "    \"\"\"\n",
    "\n",
    "    rewards = np.zeros(_total_episodes)\n",
    "    epsilons = np.zeros(_total_episodes)\n",
    "    last_states = np.zeros(_total_episodes)\n",
    "    q_averages = np.zeros(_total_episodes)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for episode in range(_total_episodes):\n",
    "\n",
    "        state = env.reset()\n",
    "        game_rewards = 0\n",
    "\n",
    "        # for each episode loop over the max number of steps that are possible\n",
    "        # take an action and observe the outcome state (new_state), reward and stopping criterion\n",
    "        for step in range(_max_game_steps):\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            agent.update_qtable(state, new_state, action, reward, done)\n",
    "            state = new_state\n",
    "            game_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards[episode] = game_rewards\n",
    "        last_states[episode] = state\n",
    "        epsilons[episode] = agent.epsilon\n",
    "        q_averages[episode] = np.sum(agent.qtable)\n",
    "\n",
    "        # reduce epsilon, for exploration-exploitation tradeoff\n",
    "        agent.update_epsilon(episode)\n",
    "\n",
    "        if episode % 300 == 0:\n",
    "            elapsed_time = round((time.time() - start), 1)\n",
    "            print(f\"elapsed time [sec]: {elapsed_time}, episode: {episode}\")\n",
    "\n",
    "    agent.rewards = rewards\n",
    "    agent.last_states = last_states\n",
    "    agent.epsilons = epsilons\n",
    "    agent.q_averages = q_averages\n",
    "    return agent\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "# Set the training parameters\n",
    "env.env.slip = 0.0  # avoid slipping in on the chain\n",
    "\n",
    "max_game_steps = 10  # Set number of steps an agent can take, before the environment is reset,\n",
    "total_episodes = 1000  # Set total of training episodes (the number of trials an agent can do)\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "b9cd3aa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:05:44.697081Z",
     "start_time": "2025-02-19T12:05:44.530897Z"
    }
   },
   "source": [
    "name = 'Smart Agent 1 - the agent explores and takes future rewards into account'\n",
    "color = \"orange\"\n",
    "\n",
    "learning_parameters = {\n",
    "    'learning_rate': 0.8,\n",
    "    'gamma': 0.9 \n",
    "}  \n",
    "exploration_parameters = {\n",
    "    'epsilon': 1,\n",
    "    'max_epsilon': 1,\n",
    "    'min_epsilon': 0.0,\n",
    "    'decay_rate': 0.008\n",
    "} \n",
    "\n",
    "q_agent_1 = Qagent(action_size, state_size, learning_parameters, exploration_parameters, name, color)\n",
    "q_agent_1 = learn_to_play(q_agent_1, _max_game_steps=max_game_steps, _total_episodes=total_episodes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time [sec]: 0.0, episode: 0\n",
      "elapsed time [sec]: 0.1, episode: 300\n",
      "elapsed time [sec]: 0.1, episode: 600\n",
      "elapsed time [sec]: 0.1, episode: 900\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "57dcc8f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:05:46.901234Z",
     "start_time": "2025-02-19T12:05:46.746286Z"
    }
   },
   "source": [
    "name = 'Greedy Agent 2 - the agent cares only about immediate rewards (small gamma)'\n",
    "color =  \"m\"\n",
    "\n",
    "learning_parameters = {\n",
    "    'learning_rate': 0.8,\n",
    "    'gamma': 0.01\n",
    "}   \n",
    "exploration_parameters = {\n",
    "    'epsilon': 1,\n",
    "    'max_epsilon': 0.5,\n",
    "    'min_epsilon': 0.0,\n",
    "    'decay_rate': 0.008\n",
    "} \n",
    "\n",
    "q_agent_2 = Qagent(action_size, state_size, learning_parameters, exploration_parameters, name, color)\n",
    "q_agent_2 = learn_to_play(q_agent_2, _max_game_steps=max_game_steps, _total_episodes=total_episodes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time [sec]: 0.0, episode: 0\n",
      "elapsed time [sec]: 0.0, episode: 300\n",
      "elapsed time [sec]: 0.1, episode: 600\n",
      "elapsed time [sec]: 0.1, episode: 900\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "31c26677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:05:48.395270Z",
     "start_time": "2025-02-19T12:05:48.238557Z"
    }
   },
   "source": [
    "name = \"Shy Agent 3 - the agent doesn't explore the environment (small epsilon)\"\n",
    "color = \"b\"\n",
    "\n",
    "learning_parameters = {\n",
    "    'learning_rate': 0.8,\n",
    "    'gamma': 0.9\n",
    "} \n",
    "exploration_parameters = {\n",
    "    'epsilon': 1,\n",
    "    'max_epsilon': 0.2,\n",
    "    'min_epsilon': 0.0,\n",
    "    'decay_rate': 0.5\n",
    "} \n",
    "\n",
    "q_agent_3 = Qagent(action_size, state_size, learning_parameters, exploration_parameters, name, color)\n",
    "q_agent_3 = learn_to_play(q_agent_3, _max_game_steps=max_game_steps, _total_episodes=total_episodes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time [sec]: 0.0, episode: 0\n",
      "elapsed time [sec]: 0.0, episode: 300\n",
      "elapsed time [sec]: 0.1, episode: 600\n",
      "elapsed time [sec]: 0.1, episode: 900\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "e6935cfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:05:54.175460Z",
     "start_time": "2025-02-19T12:05:54.172430Z"
    }
   },
   "source": [
    "# from helper_functions.visualize_plays import VisualizePlays\n",
    "# visualize the different agents\n",
    "#plays = VisualizePlays(q_agent_1, q_agent_2, q_agent_3)\n",
    "#plays.plot()"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "b2560e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:05:55.407401Z",
     "start_time": "2025-02-19T12:05:55.404525Z"
    }
   },
   "source": [
    "#!pip install helper_functions"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "11620f2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:05:56.108686Z",
     "start_time": "2025-02-19T12:05:56.104762Z"
    }
   },
   "source": [
    "print(q_agent_1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smart Agent 1 - the agent explores and takes future rewards into accountt\n",
      "╒═════════╤════════════╤════════════╕\n",
      "│         │   Action 0 │   Action 1 │\n",
      "╞═════════╪════════════╪════════════╡\n",
      "│ State 0 │      65.61 │     61.049 │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 1 │      72.9  │     61.049 │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 2 │      81    │     61.049 │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 3 │      90    │     61.049 │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 4 │     100    │     61.049 │\n",
      "╘═════════╧════════════╧════════════╛\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "a988e69e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:05:57.907613Z",
     "start_time": "2025-02-19T12:05:57.903605Z"
    }
   },
   "source": [
    "print(q_agent_2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Agent 2 - the agent cares only about immediate rewards (small gamma)\n",
      "╒═════════╤════════════╤════════════╕\n",
      "│         │   Action 0 │   Action 1 │\n",
      "╞═════════╪════════════╪════════════╡\n",
      "│ State 0 │  0.020202  │    2.0202  │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 1 │  0.019137  │    2.0202  │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 2 │  0.020202  │    1.93939 │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 3 │  0.0185613 │    2.0202  │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 4 │  0         │    2.00339 │\n",
      "╘═════════╧════════════╧════════════╛\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8db3ffb3b893135e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
