{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75abc13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the gym module that contains the NChain environment\n",
    "#!pip install gym==0.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d71d8314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0, 1, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the necessary python modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import random\n",
    "import gym\n",
    "import sys\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# ignore warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "# initialize the nchain environment\n",
    "env = gym.make('NChain-v0')\n",
    "# get 10 randomly sampled actions\n",
    "[env.action_space.sample() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70b1b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 1, 1, 0, 3, 0, 3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 10 randomly sampled states\n",
    "[env.observation_space.sample() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4171bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qagent(object):\n",
    "    \"\"\"\n",
    "    Implementation of a Q-learning Algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_size: int,\n",
    "        state_size: int,\n",
    "        learning_parameters: dict,\n",
    "        exploration_parameters: dict,\n",
    "        name: str = \"agent\",\n",
    "        color: str = \"r\",\n",
    "    ) -> None:\n",
    "        \"\"\" initialize the q-learning agent\n",
    "\n",
    "        Args:\n",
    "            action_size (int): number of actions the agent can take\n",
    "            state_size (int): numbe of states the env has\n",
    "            learning_parameters (dict): learning paramters of the agent\n",
    "            exploration_parameters (dict): exploration paramters for the agent\n",
    "            name (str, optional):  set the name of the Q-Agent. Defaults to \"agent\".\n",
    "            color (str, optional): set the color of the agent for plotting. Defaults to \"r\".\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.color = color\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        self.qtable = np.zeros((state_size, action_size))\n",
    "\n",
    "        self.learning_rate = learning_parameters[\"learning_rate\"]\n",
    "        self.gamma = learning_parameters[\"gamma\"]\n",
    "\n",
    "        self.epsilon = exploration_parameters[\"epsilon\"]\n",
    "        self.max_epsilon = exploration_parameters[\"max_epsilon\"]\n",
    "        self.min_epsilon = exploration_parameters[\"min_epsilon\"]\n",
    "        self.decay_rate = exploration_parameters[\"decay_rate\"]\n",
    "\n",
    "    def update_qtable(\n",
    "        self, state: int, new_state: int, action: int, reward: int, done: bool\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        update the q-table: Q(s,a) = Q(s,a) + lr  * [R(s,a) + gamma * max Q(s',a') - Q (s,a)]\n",
    "\n",
    "        Args:\n",
    "          state (int): current state of the environment\n",
    "          new_state (int): new state of the environment\n",
    "          action (int): current action taken by agent\n",
    "          reward (int): current reward received from env\n",
    "          done (boolean): variable indicating if env is done\n",
    "        \"\"\"\n",
    "        new_qvalue = (\n",
    "            reward\n",
    "            + self.gamma * np.max(self.qtable[new_state, :]) * (1 - done)\n",
    "            - self.qtable[state, action]\n",
    "        )\n",
    "        self.qtable[state, action] = (\n",
    "            self.qtable[state, action] + self.learning_rate * new_qvalue\n",
    "        )\n",
    "\n",
    "    def update_epsilon(self, episode: int) -> None:\n",
    "        \"\"\"\n",
    "        reduce epsilon, exponential decay\n",
    "\n",
    "        Args:\n",
    "          episode (int): number of episode\n",
    "        \"\"\"\n",
    "        self.epsilon = self.min_epsilon + (\n",
    "            self.max_epsilon - self.min_epsilon\n",
    "        ) * np.exp(-self.decay_rate * episode)\n",
    "\n",
    "    def get_action(self, state: int) -> int:\n",
    "        \"\"\"\n",
    "        select action e-greedy.exploration-exploitation trade-off:\n",
    "        - exploitation, max value for given state\n",
    "        - exploration, random choice\n",
    "\n",
    "        Args:\n",
    "          state (int): current state of the environment/agent\n",
    "\n",
    "        Returns:\n",
    "          action (int): action that the agent will take in the next step\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) >= self.epsilon:\n",
    "            action = np.argmax(self.qtable[state, :])\n",
    "        else:\n",
    "            action = np.random.choice(self.action_size)\n",
    "        return action\n",
    "\n",
    "    def __str__(self, tablefmt=\"fancy_grid\") -> None:\n",
    "        \"\"\"plot the q-table. Generate the table in fancy format.\n",
    "        \"\"\"\n",
    "        headers = [f\"Action {action}\" for action in range(self.action_size)]\n",
    "        showindex = [f\"State {state}\" for state in range(self.state_size)]\n",
    "        table = tabulate(self.qtable, headers=headers, showindex=showindex, tablefmt=\"fancy_grid\")\n",
    "        return f\"{self.name}\\n{table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc356dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_to_play(agent: Qagent, max_game_steps: int = 10, total_episodes: int = 1000) -> Qagent:\n",
    "    \"\"\"\n",
    "    implementation of the q-learning algorithm, here the q-table values are calculated\n",
    "\n",
    "    Args:\n",
    "      max_game_steps (int): number of stepts an agent can take, before the environment is reset\n",
    "      total_episodes (int): total of training episodes (the number of trials a agent can do)\n",
    "    \"\"\"\n",
    "\n",
    "    rewards = np.zeros(total_episodes)\n",
    "    epsilons = np.zeros(total_episodes)\n",
    "    last_states = np.zeros(total_episodes)\n",
    "    q_averages = np.zeros(total_episodes)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for episode in range(total_episodes):\n",
    "\n",
    "        state = env.reset()\n",
    "        game_rewards = 0\n",
    "\n",
    "        # for each episode loop over the max number of steps that are possible\n",
    "        # take an action and observe the outcome state (new_state), reward and stopping criterion\n",
    "        for step in range(max_game_steps):\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            agent.update_qtable(state, new_state, action, reward, done)\n",
    "            state = new_state\n",
    "            game_rewards += reward\n",
    "\n",
    "            if done == True:\n",
    "                break\n",
    "\n",
    "        rewards[episode] = game_rewards\n",
    "        last_states[episode] = state\n",
    "        epsilons[episode] = agent.epsilon\n",
    "        q_averages[episode] = np.sum(agent.qtable)\n",
    "\n",
    "        # reduce epsilon, for exploration-exploitation tradeoff\n",
    "        agent.update_epsilon(episode)\n",
    "\n",
    "        if episode % 300 == 0:\n",
    "            elapsed_time = round((time.time() - start), 1)\n",
    "            print(f\"elapsed time [sec]: {elapsed_time}, episode: {episode}\")\n",
    "\n",
    "    agent.rewards = rewards\n",
    "    agent.last_states = last_states\n",
    "    agent.epsilons = epsilons\n",
    "    agent.q_averages = q_averages\n",
    "    return agent\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "# Set the training parameters\n",
    "env.env.slip = 0.0  # avoid slipping in on the chain\n",
    "\n",
    "max_game_steps = 10  # Set number of stepts an agent can take, before the environment is reset, \n",
    "total_episodes = 1000  # Set total of training episodes (the number of trials a agent can do)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9cd3aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time [sec]: 0.0, episode: 0\n",
      "elapsed time [sec]: 0.1, episode: 300\n",
      "elapsed time [sec]: 0.1, episode: 600\n",
      "elapsed time [sec]: 0.1, episode: 900\n"
     ]
    }
   ],
   "source": [
    "name = 'Smart Agent 1 - the agent explores and takes future rewards into accountt'\n",
    "color = \"orange\"\n",
    "\n",
    "learning_parameters = {\n",
    "    'learning_rate': 0.8,\n",
    "    'gamma': 0.9 \n",
    "}  \n",
    "exploration_parameters = {\n",
    "    'epsilon': 1,\n",
    "    'max_epsilon': 1,\n",
    "    'min_epsilon': 0.0,\n",
    "    'decay_rate': 0.008\n",
    "} \n",
    "\n",
    "q_agent_1 = Qagent(action_size, state_size, learning_parameters, exploration_parameters, name, color)\n",
    "q_agent_1 = learn_to_play(q_agent_1, max_game_steps=max_game_steps, total_episodes=total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57dcc8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time [sec]: 0.0, episode: 0\n",
      "elapsed time [sec]: 0.0, episode: 300\n",
      "elapsed time [sec]: 0.1, episode: 600\n",
      "elapsed time [sec]: 0.1, episode: 900\n"
     ]
    }
   ],
   "source": [
    "name = 'Greedy Agent 2 - the agent cares only about immediate rewards (small gamma)'\n",
    "color =  \"m\"\n",
    "\n",
    "learning_parameters = {\n",
    "    'learning_rate': 0.8,\n",
    "    'gamma': 0.01\n",
    "}   \n",
    "exploration_parameters = {\n",
    "    'epsilon': 1,\n",
    "    'max_epsilon': 0.5,\n",
    "    'min_epsilon': 0.0,\n",
    "    'decay_rate': 0.008\n",
    "} \n",
    "\n",
    "q_agent_2 = Qagent(action_size, state_size, learning_parameters, exploration_parameters, name, color)\n",
    "q_agent_2 = learn_to_play(q_agent_2, max_game_steps=max_game_steps, total_episodes=total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31c26677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time [sec]: 0.0, episode: 0\n",
      "elapsed time [sec]: 0.0, episode: 300\n",
      "elapsed time [sec]: 0.1, episode: 600\n",
      "elapsed time [sec]: 0.1, episode: 900\n"
     ]
    }
   ],
   "source": [
    "name = \"Shy Agent 3 - the agent doesn't explore the environment (small epsilon)\"\n",
    "color = \"b\"\n",
    "\n",
    "learning_parameters = {\n",
    "    'learning_rate': 0.8,\n",
    "    'gamma': 0.9\n",
    "} \n",
    "exploration_parameters = {\n",
    "    'epsilon': 1,\n",
    "    'max_epsilon': 0.2,\n",
    "    'min_epsilon': 0.0,\n",
    "    'decay_rate': 0.5\n",
    "} \n",
    "\n",
    "q_agent_3 = Qagent(action_size, state_size, learning_parameters, exploration_parameters, name, color)\n",
    "q_agent_3 = learn_to_play(q_agent_3, max_game_steps=max_game_steps, total_episodes=total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b339d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6935cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from helper_functions.visualize_plays import VisualizePlays  # \n",
    "# visualize the different agents\n",
    "#plays = VisualizePlays(q_agent_1, q_agent_2, q_agent_3)\n",
    "#plays.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2560e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install helper_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11620f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smart Agent 1 - the agent explores and takes future rewards into accountt\n",
      "╒═════════╤════════════╤════════════╕\n",
      "│         │   Action 0 │   Action 1 │\n",
      "╞═════════╪════════════╪════════════╡\n",
      "│ State 0 │      65.61 │     61.049 │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 1 │      72.9  │     61.049 │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 2 │      81    │     61.049 │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 3 │      90    │     61.049 │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 4 │     100    │     61.049 │\n",
      "╘═════════╧════════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "print(q_agent_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a988e69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Agent 2 - the agent cares only about immediate rewards (small gamma)\n",
      "╒═════════╤════════════╤════════════╕\n",
      "│         │   Action 0 │   Action 1 │\n",
      "╞═════════╪════════════╪════════════╡\n",
      "│ State 0 │  0.020202  │    2.0202  │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 1 │  0.020202  │    2.0202  │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 2 │  0.0131098 │    2.0202  │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 3 │  0.100166  │    1.61616 │\n",
      "├─────────┼────────────┼────────────┤\n",
      "│ State 4 │ 10.101     │    1.93939 │\n",
      "╘═════════╧════════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "print(q_agent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82324ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
